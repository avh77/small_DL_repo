{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Бустинг </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### План\n",
    "\n",
    "1. #### Теория\n",
    "    - пример для иллюстрации принципа работы бустинга\n",
    "    - библиотеки: Sklearn, XgBoost, LightGBM, CatBoost\n",
    "2. #### Практика\n",
    "    - Пример запуска sklearn.ensemble.GradientBoostingClassifier\n",
    "    - Пример запуска LightGBM\n",
    "    - Сравнение GradientBoostingClassifier и LightGBM\n",
    "3. #### Выводы\n",
    "4. #### Дополнительный материал: соревнование Porto Seguro Safe Driver's Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/boosting.jpg' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример для иллюстрации принципа работы бустинга\n",
    "Пусть базовые алгоритмы – всего лишь \"пеньки\", то есть деревья решений глубины 1.\n",
    "<img src='img/adaboost_toy_step1.png' width=700>\n",
    "Веса объектов, на которых базовый алгоритм ошибается, увеличиваются (кружки увеличиваются в размере).\n",
    "<img src='img/adaboost_toy_step2.png' width=700>\n",
    "В конце базовые алгоритмы \"голосуют\", их веса определялись $\\alpha_t$ в процессе построения.\n",
    "<img src='img/adaboost_toy_step3.png' width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/timeline.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XgBoost\n",
    "- есть JVM, Julia, R package\n",
    "- не умеет обрабатывать категориальные признаки (в отличие от LightGBM и CatBoost)\n",
    "- зачастую ставится на Windows с большими проблемами\n",
    "\n",
    "#### LightGBM\n",
    "- умеет обрабатывать категориальные признаки\n",
    "- эффективно расходует память\n",
    "- достаточно часто дает качество чуть лучше, чем CatBoost\n",
    " \n",
    "#### CatBoost\n",
    "- можно добавить регуляризацию (помогает избежать переобучения)\n",
    "- есть C и C++ API\n",
    "- можно использовать красивые средства визуализации: CatBoost Viewer и TensorBoard\n",
    "- можно прикрутить к ClickHouse\n",
    "- feature/object importance (можно считать важность не только признаков, но и объектов в выборке)\n",
    "- делает бэкапы в процессе обучения (если что-то пойдет не так, не нужно будет переучивать модель заново)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В sklearn доступны алгоритмы AdaBoost и GradientBoosting для задач классификации и регрессии.\n",
    "Рассмотрим пример запуска <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\">GradientBoostingClassifier</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Пример запуска sklearn.ensemble.GradientBoostingClassifier\n",
    "### <a href='http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html'> Список параметров </a>\n",
    "- #### метрики\n",
    "    - loss [default=\"deviance\"] - оптимизируемая функция потерь.  Одна из {\"deviance\", \"exponential\"}. Первая соответствует логистической регрессии и возвращает вероятности, вторая - AdaBoost.\n",
    "- #### процесс обучения\n",
    "    - learning_rate [default=0.1] - темп обучения, насколько быстро будут меняться веса.\n",
    "    - n_estimators [default=100] - число итераций градиентного бустинга.\n",
    "- #### характеристики деревьев в композиции\n",
    "    - max_depth [default=3] - максимальная глубина деревьев в композиции.\n",
    "    - min_samples_split [default=2] - минимальное число примеров, необходимое для разветвления в данной вершине.\n",
    "    - min_samples_leaf [default=1] - минимальное число примеров в листе.\n",
    "    - min_weight_fraction_leaf [default=0.0] - минимальное взвешенное число примеров в листе.\n",
    "- #### технические параметры\n",
    "    - verbose [default=0] - нужно ли печатать отладочный вывод или нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерирация датасета для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем генерировать выборку из 1000 объектов и 50 признаков с помощью `sklearn.datasets.make_classification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=50, \n",
    "                           n_informative=20)\n",
    "\n",
    "# splitting dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GradientBoostingClassifier initialization\n",
    "clf = GradientBoostingClassifier(n_estimators=100, max_depth=5)\n",
    "\n",
    "# GradientBoostingClassifier fitting\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предсказание меток классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# measure accuracy\n",
    "accuracy = accuracy_score(y_pred, y_test)    \n",
    "\n",
    "# print accuracy\n",
    "print(u'Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Пример запуска LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href='https://lightgbm.readthedocs.io/en/latest/Parameters.html'> Список параметров </a>\n",
    "- #### метрики\n",
    "    - objective [default=\"regression\"] - оптимизируемая функция потерь. Одна из:\n",
    "        - regression\n",
    "        - regression_l1\n",
    "        - huber\n",
    "        - fair\n",
    "        - poisson\n",
    "        - quantile\n",
    "        - mape\n",
    "        - binary\n",
    "        - multiclass\n",
    "        - xentropy\n",
    "        - lambdarank\n",
    "        - etc.\n",
    "- #### процесс обучения\n",
    "    - learning_rate [default=0.1] - темп обучения, насколько быстро будут меняться веса.\n",
    "    - num_iteration [default=100] - число итераций градиентного бустинга.\n",
    "- #### характеристики деревьев в композиции\n",
    "    - max_depth [default=-1 (нет ограничения на глубину)] - максимальная глубина деревьев в композиции.\n",
    "    - min_gain_to_split [default=0.0] - минимальное значение критерия информативности, необходимое для разветвления в данной вершине.\n",
    "    - min_data_in_leaf [default=20] - минимальное число примеров в листе.\n",
    "- #### технические параметры\n",
    "    - verbosity [default=1] - нужно ли печатать отладочный вывод или нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерирация датасета для обучения (аналогично предыдущему примеру)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=50, n_informative=20)\n",
    "\n",
    "# splitting dataset into train and test\n",
    "X_train, X_test, y_train, y_test = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед обучением нужно превратить данные в формат, доступный в LightGBM. Делаем это с помощью команды `lightgbm.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LightGBM data containers\n",
    "train_data = lightgbm.Dataset(\n",
    "    X_train,\n",
    "    label=y_train\n",
    ")\n",
    "\n",
    "test_data = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lightgbm.train(\n",
    "    {'num_iterations': 100, 'max_depth': 5},\n",
    "    train_data,\n",
    "    valid_sets=test_data,\n",
    "    num_boost_round=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предсказание меток классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.95728359,  0.70460998,  0.18385195, -0.00568949,  0.89636035])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на `y_pred`. В отличие от `GradientBoostingClassifier` из `sklearn`, `LightGBM` предсказывает не класс объекта, а его вероятность принадлежности классу. Перед тем, как считать `accuracy`, посчитаем метки классов по вероятностям (считаем, что если вероятность больше $0.5$, то объект принадлежит классу $1$, иначе классу $0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform probabilities to labels\n",
    "def binarize(prob):\n",
    "    return int(prob >= 0.5)\n",
    "\n",
    "y_pred = list(map(binarize, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.805\n"
     ]
    }
   ],
   "source": [
    "# measure accuracy\n",
    "accuracy = # your code here  \n",
    "\n",
    "# print accuracy\n",
    "print(u'Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "Основные преимущества LightGBM по сравнению с sklearn.ensembles.GradientBoostingClassifier:\n",
    "- Помимо деревьев возможно использование линейных моделей в качестве базовых классификаторов.\n",
    "- Скорость работы (cм. `boosting_solved_with_extra_materials.ipynb`)\n",
    "- Возможность распараллеливания.\n",
    "- Значительно больший выбор стандартных функций потерь, а также возможность задавать свою функцию потерь.\n",
    "- Наличие регуляризаторов в итоговой функции потерь и возможность задавать их коэффициенты, что даёт еще один метод борьбы с переобучением, помимо использования случайности (subsample, colsample_bytree) и основных параметров дерева решений.\n",
    "- Встроенная обработка missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полезные ссылки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <a href='https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db'> Сравнение XgBoost, LightGBM и CatBoost </a>\n",
    "2. <a href='http://xgboost.readthedocs.io'> Документация XgBoost </a>\n",
    "3. <a href='https://lightgbm.readthedocs.io/en/latest/'> Документация LightGBM </a>\n",
    "4. <a href='https://tech.yandex.com/catboost/doc/dg/concepts/about-docpage/'> Документация CatBoost </a>\n",
    "5. <a href='http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html'> Документация sklearn.ensemble.GradientBoostingClassifier </a>\n",
    "6. <a href='https://habr.com/company/ods/blog/327250/'> Статья ODS про бустинг </a>\n",
    "7. <a href='https://alexanderdyakonov.wordpress.com/2017/06/09/градиентный-бустинг/comment-page-1/'> Статья А.Дьяконова про градиентный бустинг </a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 research env",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
